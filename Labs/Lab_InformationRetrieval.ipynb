{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5iMsYaqQc_k"
      },
      "source": [
        "Individually Graded: ***Due date: April 15***\n",
        "\n",
        "\n",
        "**Submission Intruction:**\n",
        "Demo the lab to your TA/instructor and then upload your Jupyter Notebook to Canvas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRrMIN-V1ON2"
      },
      "source": [
        "**Lab Assignment: Text Processing and Document Analysis**\n",
        "\n",
        "**Objective:**\n",
        "The objective of this lab assignment is to practice text processing techniques and document analysis using Python. You will use the file: documents.txt that can be accessed: https://raw.githubusercontent.com/sumonacalpoly/Datasets/main/documents.txt\n",
        "\n",
        "**Tasks:**\n",
        "1. Implement a `TextVector` class that represents a vector of text data. This class should provide methods for adding words to the vector, calculating word frequencies, and retrieving various statistics about the text data.\n",
        "2. Implement a `DocumentCollection` class that represents a collection of documents. This class should be able to add documents, serialize and deserialize the collection, and provide methods for analyzing the text data within the documents.\n",
        "3. Create a function to extract data from a text file containing multiple documents. This function should parse the document metadata and text bodies, and populate a `DocumentCollection` object with the extracted data.\n",
        "4. Utilize the `requests` library to fetch the text data from a URL and process it using the implemented classes and functions.\n",
        "5. Implement a `main` function to orchestrate the entire process, including fetching the data, processing it, and analyzing it.\n",
        "6. Serialize the resulting `DocumentCollection` object to a file for future use.\n",
        "\n",
        "**Lab Tasks:**\n",
        "1. Implement the `TextVector` class as described above.\n",
        "2. Implement the `DocumentCollection` class as described above.\n",
        "3. Write a function to extract data from a text file (`extract_data`). This function should take the text content of the file as input and return a `DocumentCollection` object populated with the extracted data.\n",
        "4. Implement the `main` function to fetch the text data from a given URL, process it using the `extract_data` function, and perform analysis on the extracted documents.\n",
        "5. Serialize the resulting `DocumentCollection` object to a file named `document_collection.pkl`.\n",
        "6. Write test cases to verify the functionality of the implemented classes and functions.\n",
        "\n",
        "Your implementation should give the following output:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Word = is\n",
        "Frequency = 31\n",
        "Distinct Number of Words = 103509\n",
        "Total word count = 142867\n",
        "Raw vector entry set: dict_items([('hello', 2), ('world', 1), ('python', 1)])\n",
        "Contains 'hello': True\n",
        "Frequency of 'hello': 2\n",
        "Total word count: 4\n",
        "Distinct word count: 3\n",
        "Highest raw frequency: 2\n",
        "Most frequent word: ('hello', 2)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Submission:**\n",
        "Submit the following files:\n",
        "- Python script containing the implementation of the `TextVector` and `DocumentCollection` classes, the `extract_data` function, and the `main` function.\n",
        "Complete all the modules and functions. You can add extra functions or modify the given code as per your understanding.\n",
        "\n",
        "**Note:** Ensure proper documentation and comments are provided throughout the code for clarity and understanding.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMVjvU6sP--q"
      },
      "source": [
        "To complete this task, please refer to the Jupyter notebook tutorial: Tutorial_3_Objects_Classes_Python.ipynb on how to create objects and classes if you have forgotten or it is new to you.\n",
        "\n",
        "This task has helper code for some of the components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "So5jMXAGQby-"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIo3ZYacRoyR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-aCyYeSz-QC"
      },
      "source": [
        "Your implementation should have two classes: class TextVector and class DocumentCollection.\n",
        "\n",
        "The class TextVector should have the following methods:\n",
        "This `TextVector` class provides methods for working with a collection of words. Here's what each method does:\n",
        "\n",
        "1. `__init__(self)`: Initializes a new instance of `TextVector`. It initializes two attributes: `word_freq`, which is a defaultdict that stores the frequency of each word, and `text`, which stores the concatenated text.\n",
        "\n",
        "2. `add_word(self, word)`: Adds a word to the `word_freq` dictionary and appends the word to the `text` attribute if its length is greater than or equal to 2.\n",
        "\n",
        "3. `getRawVectorEntrySet(self)`: Returns the items (word, frequency) in the `word_freq` dictionary.\n",
        "\n",
        "4. `add(self, word)`: A convenience method to add a word using the `add_word` method.\n",
        "\n",
        "5. `contains(self, word)`: Checks if a word is present in the `word_freq` dictionary. It converts the word to lowercase before checking.\n",
        "\n",
        "6. `getRawFrequency(self, word)`: Returns the frequency of a word in the `word_freq` dictionary. It converts the word to lowercase before retrieving its frequency.\n",
        "\n",
        "7. `getTotalWordCount(self)`: Returns the total count of words in the `word_freq` dictionary, which is the sum of all word frequencies.\n",
        "\n",
        "8. `getDistinctWordCount(self)`: Returns the count of distinct words in the `word_freq` dictionary.\n",
        "\n",
        "9. `getHighestRawFrequency(self)`: Returns the highest frequency of any word in the `word_freq` dictionary. If the dictionary is empty, it returns 0.\n",
        "\n",
        "This class provides basic functionality to work with word frequencies and text statistics.\n",
        "\n",
        "These are the basic modules that your implementation MUST have, but you can have other modules as well.\n",
        "\n",
        "The other class: ***class DocumentCollection***\n",
        " represents a collection of documents, where each document is represented as a TextVector.\n",
        "\n",
        "--The add_document method adds a document to the collection. It splits the text into words, adds each word to the corresponding TextVector, and updates the collection.\n",
        "\n",
        "--The serialize method serializes the collection to a file. (Helper code provided)\n",
        "\n",
        "--The deserialize method deserializes a collection from a file. (Helper code provided)\n",
        "\n",
        "The other functions outside the classes are:\n",
        "\n",
        "***extract_data function:***\n",
        "\n",
        "This function extracts data from a text and creates a DocumentCollection instance.\n",
        "It iterates through the text lines, identifies document boundaries, and adds each document to the collection using the add_document method.\n",
        "\n",
        "***main function***:\n",
        "\n",
        "This is the entry point of the script.\n",
        "It retrieves text data from a URL, creates a DocumentCollection using the extract_data function, and performs some analysis on the text data.\n",
        "It prints the most frequent word, its frequency, the distinct number of words, and the total word count.\n",
        "Finally, it serializes the DocumentCollection instance to a file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bFoOXRgQhTJf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total word count: 2\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import re\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "class TextVector:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize a TextVector instance.\n",
        "        \"\"\"\n",
        "        self.word_freq = defaultdict(int)\n",
        "        self.text = \"\"\n",
        "\n",
        "    def add_word(self, word):\n",
        "        \"\"\"\n",
        "        Add a word to the TextVector instance.\n",
        "        Parameters:\n",
        "            word (str): The word to add.\n",
        "        \"\"\"\n",
        "        # WRITE YOUR CODE HERE TO COMPLETE THIS MODULE\n",
        "\n",
        "        if len(word) >= 2:\n",
        "            self.word_freq[word.lower()] += 1\n",
        "            self.text += word + \" \"\n",
        "\n",
        "    def getRawVectorEntrySet(self):\n",
        "        \"\"\"\n",
        "        Get the raw vector entry set of the TextVector instance.\n",
        "        Returns:\n",
        "            dict_items: The items (word, frequency) in the word_freq dictionary.\n",
        "        \"\"\"\n",
        "        # WRITE YOUR CODE HERE TO COMPLETE THIS MODULE\n",
        "\n",
        "        return self.word_freq.items()\n",
        "\n",
        "    def add(self, word):\n",
        "        \"\"\"\n",
        "        Alias for add_word method.\n",
        "        Parameters:\n",
        "            word (str): The word to add.\n",
        "        \"\"\"\n",
        "        # WRITE YOUR CODE HERE TO COMPLETE THIS MODULE\n",
        "\n",
        "        self.add_word(word)\n",
        "\n",
        "    def contains(self, word):\n",
        "        \"\"\"\n",
        "        Check if the word is present in the TextVector instance.\n",
        "        Parameters:\n",
        "            word (str): The word to check.\n",
        "        Returns:\n",
        "            bool: True if the word is present, False otherwise.\n",
        "        \"\"\"\n",
        "        # WRITE YOUR CODE HERE TO COMPLETE THIS MODULE\n",
        "\n",
        "        return word.lower() in self.word_freq\n",
        "\n",
        "    def getRawFrequency(self, word):\n",
        "        \"\"\"\n",
        "        Get the frequency of a word in the TextVector instance.\n",
        "        Parameters:\n",
        "            word (str): The word to get frequency for.\n",
        "        Returns:\n",
        "            int: The frequency of the word.\n",
        "        \"\"\"\n",
        "        # WRITE YOUR CODE HERE TO COMPLETE THIS MODULE\n",
        "\n",
        "        return self.word_freq[word.lower()]\n",
        "\n",
        "    def getTotalWordCount(self):\n",
        "        \"\"\"\n",
        "        Get the total word count in the TextVector instance.\n",
        "        Returns:\n",
        "            int: The total word count.\n",
        "        \"\"\"\n",
        "        # WRITE YOUR CODE HERE TO COMPLETE THIS MODULE\n",
        "\n",
        "        return sum(self.word_freq.values())\n",
        "\n",
        "    def getDistinctWordCount(self):\n",
        "        \"\"\"\n",
        "        Get the count of distinct words in the TextVector instance.\n",
        "        Returns:\n",
        "            int: The count of distinct words.\n",
        "        \"\"\"\n",
        "        # WRITE YOUR CODE HERE TO COMPLETE THIS MODULE\n",
        "\n",
        "        return len(self.word_freq)\n",
        "\n",
        "    def getHighestRawFrequency(self):\n",
        "        \"\"\"\n",
        "        Get the highest raw frequency of any word in the TextVector instance.\n",
        "        Returns:\n",
        "            int: The highest raw frequency.\n",
        "            #Write the code to return the word(s) with the highest frequency in the text vector along with their frequency.\n",
        "        # first checks if the word_freq dictionary is empty. If it's empty, it returns None, indicating that there are no words in the text vector.\n",
        "\n",
        "        #If the word_freq dictionary is not empty, it proceeds to find the word(s) with the highest frequency.\n",
        "\n",
        "        #find the maximum frequency value in the word_freq dictionary using the max function.\n",
        "\n",
        "        #then create a list most_freq_words using a list comprehension.\n",
        "        #This list comprehension iterates over the items (word, frequency) in the word_freq\n",
        "        #dictionary and selects only those words where the frequency is equal to the maximum frequency found in the previous step.\n",
        "\n",
        "        #Finally, return the first word in the most_freq_words list along with its frequency, as a tuple (word, frequency).\n",
        "        #example: return most_freq_words[0], max_freq\n",
        "        \"\"\"\n",
        "        # WRITE YOUR CODE HERE TO COMPLETE THIS MODULE\n",
        "\n",
        "        if not self.word_freq:\n",
        "            return 0\n",
        "        return max(self.word_freq.values())\n",
        "\n",
        "    # You can add more methods here...\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "# Initialize a TextVector instance\n",
        "vector = TextVector()\n",
        "\n",
        "# Add words to the TextVector instance\n",
        "vector.add(\"hello\")\n",
        "vector.add(\"world\")\n",
        "\n",
        "# Get the total word count\n",
        "total_word_count = vector.getTotalWordCount()\n",
        "\n",
        "# Print the total word count\n",
        "print(\"Total word count:\", total_word_count)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EzP30Ur6x9_"
      },
      "source": [
        "The following class description is provided below:\n",
        "\n",
        "1. **`DocumentCollection` Class**:\n",
        "   - This class represents a collection of documents.\n",
        "\n",
        "2. **`__init__` Method**:\n",
        "   - Initializes an empty `DocumentCollection`. It initializes an attribute `documents` as an empty dictionary to store documents.\n",
        "\n",
        "3. **`add_document` Method**:\n",
        "   - This method adds a document to the collection.\n",
        "   - Parameters:\n",
        "     - `doc_id` (int): The identifier for the document.\n",
        "     - `text` (str): The text content of the document.\n",
        "   - It splits the text into words using a regular expression.\n",
        "   - It creates a `TextVector` object for the document and adds each word to it using the `add_word` method of `TextVector`.\n",
        "\n",
        "4. **`serialize` Method**:\n",
        "   - Serializes the `DocumentCollection` to a file using pickle.\n",
        "   - Parameters:\n",
        "     - `filename` (str): The name of the file to which the collection will be serialized.\n",
        "   - It opens the file in binary write mode and dumps the object into the file using `pickle.dump`.\n",
        "\n",
        "5. **`deserialize` Method**:\n",
        "   - Deserializes a `DocumentCollection` from a file using pickle.\n",
        "   - Parameters:\n",
        "     - `filename` (str): The name of the file from which the collection will be deserialized.\n",
        "   - It opens the file in binary read mode and loads the object from the file using `pickle.load`.\n",
        "\n",
        "Overall, this code provides a basic structure for managing a collection of documents. It allows adding documents, serializing the collection to a file, and deserializing the collection from a file. It uses the `TextVector` class to represent the text content of each document.\n",
        "\n",
        "Complete the following code to finish implementing the DocumentCollection class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wbJBxl-IhTGC"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "class DocumentCollection:\n",
        "    def __init__(self):\n",
        "        self.documents = {}\n",
        "\n",
        "    def add_document(self, doc_id, text):\n",
        "        \"\"\"\n",
        "        Adds a document to the collection.\n",
        "\n",
        "        Parameters:\n",
        "            doc_id (int): The identifier for the document.\n",
        "            text (str): The text content of the document.\n",
        "        \"\"\"\n",
        "        # Split text into words using regular expression\n",
        "        # you can modify this code to fit your code and variables\n",
        "        self.documents[doc_id] = TextVector()\n",
        "        words = re.split(\"[^a-zA-Z]+\", text)\n",
        "\n",
        "        # Write a for loop that is responsible for processing each word in the text and adding it to the word\n",
        "        #frequency dictionary of the `TextVector` object associated with the current document.\n",
        "        # The for loop in the `add_document` method iterates over each word in the `words` list,\n",
        "        #which contains the result of splitting the `text` parameter using a regular expression pattern.\n",
        "\n",
        "        #Here's a breakdown of what happens inside the loop:\n",
        "\n",
        "        #1. **Iterate over each word:** The loop iterates over each word extracted from the text.\n",
        "\n",
        "        #2. **Add word to TextVector object:** For each word, it calls the `add_word` method of the `TextVector` object associated with the current document (`doc_id`).\n",
        "        #This method is responsible for adding the word to the word frequency dictionary within the `TextVector` object.\n",
        "\n",
        "\n",
        "         #WRITE YOUR CODE TO IMPLEMENT THE FOR LOOP\n",
        "\n",
        "        for word in words:\n",
        "           self.documents[doc_id].add_word(word)\n",
        "\n",
        "    def serialize(self, filename):\n",
        "        \"\"\"\n",
        "        Serializes the DocumentCollection to a file.\n",
        "\n",
        "        Parameters:\n",
        "            filename (str): The name of the file to which the collection will be serialized.\n",
        "        \"\"\"\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(self, f)\n",
        "\n",
        "    @staticmethod\n",
        "    def deserialize(filename):\n",
        "        \"\"\"\n",
        "        Deserializes a DocumentCollection from a file.\n",
        "\n",
        "        Parameters:\n",
        "            filename (str): The name of the file from which the collection will be deserialized.\n",
        "\n",
        "        Returns:\n",
        "            DocumentCollection: The deserialized DocumentCollection object.\n",
        "        \"\"\"\n",
        "        with open(filename, 'rb') as f:\n",
        "            return pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRXgppJI9WzQ"
      },
      "source": [
        "The following code for the function ***extract_data()*** has already been implemented for you. You can modify the code or make your own code based on your logic to do the same task. Here is the explanation of what this function does:\n",
        "This code defines a function `extract_data` that reads text data formatted in a specific way and extracts documents from it. Each document is expected to have an identifier (starting with \".I\") and content (starting with \".W\"). The function then creates a `DocumentCollection` object and adds each document to it using the `add_document` method.\n",
        "\n",
        "Here's a step-by-step explanation:\n",
        "\n",
        "1. **Define `extract_data` function:** This function takes a string `text` as input and returns a `DocumentCollection` object containing the extracted documents.\n",
        "\n",
        "2. **Initialize `DocumentCollection`:** Inside the function, a new `DocumentCollection` object named `collection` is created.\n",
        "\n",
        "3. **Iterate over lines in text:** The function splits the input `text` into lines and iterates over each line.\n",
        "\n",
        "4. **Identify document start:** If a line starts with \".I\" (indicating the start of a new document), the function checks if there is an ongoing document (i.e., if `doc_id` is not None). If so, it adds the current document to the collection using `add_document` method and resets the `text_buffer` variable.\n",
        "\n",
        "5. **Extract document ID:** The document ID is extracted from the line starting with \".I\" and stored in `doc_id`.\n",
        "\n",
        "6. **Extract document content:** If a line starts with \".W\" (indicating the start of document content), the function extracts the content by removing the \".W\" and appends it to `text_buffer`.\n",
        "\n",
        "7. **Continuously append lines to text_buffer:** If the line does not start with \".I\" or \".W\", it is considered part of the document content. In this case, the line is stripped of leading and trailing whitespaces and added to `text_buffer`.\n",
        "\n",
        "8. **Add last document:** After processing all lines, if there was an ongoing document (`doc_id` is not None), the function adds the last document to the collection.\n",
        "\n",
        "9. **Retrieve text data from URL:** The code retrieves the text data from a URL using the requests library.\n",
        "\n",
        "10. **Process text data:** If the request is successful (status code 200), the text data is extracted from the response.\n",
        "\n",
        "11. **Extract documents:** The `extract_data` function is called with the extracted text data, resulting in a `DocumentCollection` object containing the extracted documents. If the request fails, an error message is printed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IpbN2h1t6ItO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400])\n"
          ]
        }
      ],
      "source": [
        "def extract_data(text):\n",
        "    collection = DocumentCollection()\n",
        "    doc_id = None\n",
        "    text_buffer = \"\"\n",
        "    for line in text.splitlines():\n",
        "        if line.startswith(\".I\"):\n",
        "            if doc_id is not None:\n",
        "                collection.add_document(doc_id, text_buffer)\n",
        "                text_buffer = \"\"\n",
        "            doc_id = int(line.strip().split()[-1])\n",
        "        elif line.startswith(\".W\"):\n",
        "            text_buffer += line.strip().replace(\".W\", \"\")\n",
        "        else:\n",
        "            text_buffer += line.strip()\n",
        "    if doc_id is not None:  # Add the last document\n",
        "        collection.add_document(doc_id, text_buffer)\n",
        "    return collection\n",
        "\n",
        "# URL to the raw text file on GitHub\n",
        "url = 'https://raw.githubusercontent.com/sumonacalpoly/Datasets/main/documents.txt'\n",
        "\n",
        "# Get the content of the file\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Get the text content of the file\n",
        "    text = response.text\n",
        "\n",
        "    # Now you can proceed with processing the text data\n",
        "    collection = extract_data(text)\n",
        "else:\n",
        "    print(\"Failed to retrieve the file. Status code:\", response.status_code)\n",
        "\n",
        "# collection.serialize(\"C:/Users/Andrew/Documents/CSC 466/Labs/Test_Serialize.pkl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mP3NN489wWM"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCITX2Ur-gYQ"
      },
      "source": [
        "Complete the ***main()*** function. This `main` function is responsible for processing the text data stored in the `collection` object, calculating some statistics, and serializing the `DocumentCollection` object to a file named \"document_collection.pkl\". Here's a breakdown of what the function does:\n",
        "\n",
        "1. **Define Noise Words Array:** A list named `noise_word_array` is defined, containing common noise words that are typically not relevant for analysis. The \"noise word array\" in this context refers to a list of common words that are often considered noise or stop words in natural language processing tasks. These are words that occur frequently in a language but typically do not carry much meaningful information about the content of the text. Including them in text analysis tasks can sometimes add noise to the results without providing much value.\n",
        "\n",
        "In the provided code, the `noise_word_array` is a list of such words, including common articles, conjunctions, prepositions, and other frequently occurring words that are unlikely to be useful for the analysis being performed. These words are filtered out during text processing to focus on more meaningful content.\n",
        "\n",
        "2. **Initialize Variables:** Initialize variables `word_freq_sum`, `distinct_words_sum`, and `highest_freq_word`. These variables will be used to calculate statistics about the text data.\n",
        "\n",
        "3. **Iterate Over Documents:** Iterate over each document in the `collection.documents` dictionary. For each document:\n",
        "\n",
        "    a. **Iterate Over Words:** Iterate over each word-frequency pair in the `word_freq` dictionary of the `TextVector` object associated with the document.\n",
        "    \n",
        "    b. **Check Noise Words:** Check if the word is not in the `noise_word_array`. If it's not a noise word:\n",
        "    \n",
        "        - Increment `word_freq_sum` by the frequency of the word.\n",
        "        - Increment `distinct_words_sum` by 1.\n",
        "        - Check if the frequency of the word is higher than the frequency of the current `highest_freq_word`. If so, update `highest_freq_word` to this word.\n",
        "        \n",
        "4. **Print Statistics:** Print the following statistics:\n",
        "    \n",
        "    - The word with the highest frequency (`highest_freq_word`).\n",
        "    - The frequency of the highest frequency word.\n",
        "    - The total number of distinct words (`distinct_words_sum`).\n",
        "    - The total word count (`word_freq_sum`).\n",
        "\n",
        "5. **Serialize Collection:** Serialize the `DocumentCollection` object `collection` to a file named \"document_collection.pkl\" using the `serialize` method.\n",
        "\n",
        "6. **Main Function:** If this script is executed as the main program, the `main` function is called to execute the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "TpvEReYU6Jnp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Word Count: 142867\n",
            "Distinct word count: 21859\n",
            "Highest raw frequency: 31\n",
            "Most frequent word: is\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    noise_word_array = [\"a\", \"about\", \"above\", \"all\", \"along\",\n",
        "        \"also\", \"although\", \"am\", \"an\", \"and\", \"any\", \"are\", \"aren't\", \"as\", \"at\",\n",
        "        \"be\", \"because\", \"been\", \"but\", \"by\", \"can\", \"cannot\", \"could\", \"couldn't\",\n",
        "        \"did\", \"didn't\", \"do\", \"does\", \"doesn't\", \"e.g.\", \"either\", \"etc\", \"etc.\",\n",
        "        \"even\", \"ever\", \"enough\", \"for\", \"from\", \"further\", \"get\", \"gets\", \"got\", \"had\", \"have\",\n",
        "        \"hardly\", \"has\", \"hasn't\", \"having\", \"he\", \"hence\", \"her\", \"here\",\n",
        "        \"hereby\", \"herein\", \"hereof\", \"hereon\", \"hereto\", \"herewith\", \"him\",\n",
        "        \"his\", \"how\", \"however\", \"i\", \"i.e.\", \"if\", \"in\", \"into\", \"it\", \"it's\", \"its\",\n",
        "        \"me\", \"more\", \"most\", \"mr\", \"my\", \"near\", \"nor\", \"now\", \"no\", \"not\", \"or\", \"on\", \"of\", \"onto\",\n",
        "        \"other\", \"our\", \"out\", \"over\", \"really\", \"said\", \"same\", \"she\",\n",
        "        \"should\", \"shouldn't\", \"since\", \"so\", \"some\", \"such\",\n",
        "        \"than\", \"that\", \"the\", \"their\", \"them\", \"then\", \"there\", \"thereby\",\n",
        "        \"therefore\", \"therefrom\", \"therein\", \"thereof\", \"thereon\", \"thereto\",\n",
        "        \"therewith\", \"these\", \"they\", \"this\", \"those\", \"through\", \"thus\", \"to\",\n",
        "        \"too\", \"under\", \"until\", \"unto\", \"upon\", \"us\", \"very\", \"was\", \"wasn't\",\n",
        "        \"we\", \"were\", \"what\", \"when\", \"where\", \"whereby\", \"wherein\", \"whether\",\n",
        "        \"which\", \"while\", \"who\", \"whom\", \"whose\", \"why\", \"with\", \"without\",\n",
        "        \"would\", \"you\", \"your\", \"yours\", \"yes\"]\n",
        "\n",
        "    #Write a block of code  to\n",
        "    #analyzing the text data to find the highest frequency word, along with\n",
        "    #some additional statistics about the word frequencies and the number of distinct words.\n",
        "    #Hint: You must have  a loop to iterate over each document\n",
        "    #in the collection (collection.documents).\n",
        "    #For each document, it iterates over each word and its\n",
        "    #frequency in the word_freq dictionary within the TextVector object associated with that document.\n",
        "    #The guideline is provided here:\n",
        "\n",
        "\n",
        "    #1. **Looping through documents and words:**\n",
        "    #  - The outer loop iterates over each document in the collection.\n",
        "    # - The inner loop iterates over each word and its frequency in the current document.\n",
        "\n",
        "    #2. **Checking for noise words:**\n",
        "     # - Inside the inner loop, there's a condition that checks if the current word is not in the `noise_word_array`.\n",
        "     # - `noise_word_array` seems to contain a list of common noise words that are typically filtered out in text analysis tasks (e.g., \"the\", \"and\", \"or\", etc.).\n",
        "\n",
        "    #3. **Updating word frequency and distinct word count:**\n",
        "     # - If the current word is not a noise word, its frequency (`freq`) is added to the `word_freq_sum`. This variable seems to accumulate the total frequency of all non-noise words in the collection.\n",
        "     # - Similarly, the `distinct_words_sum` variable is incremented by 1 for each non-noise word encountered, representing the total count of distinct words in the collection.\n",
        "\n",
        "    #4. **Finding the highest frequency word:**\n",
        "\n",
        "    #5. **Printing results:**\n",
        "      # print out information about the highest frequency word (`highest_freq_word`) and its frequency.\n",
        "      # also print the distinct number of words encountered (`distinct_words_sum`) and the total word count (`word_freq_sum`).\n",
        "\n",
        "\n",
        "    # NOW WRITE YOUR CODE HERE\n",
        "\n",
        "    # Calc highest freqs\n",
        "    highest_freq_word = [\"\", 0]\n",
        "    \n",
        "    # Calc distinct words\n",
        "    distinct_words_sum = 0\n",
        "    all_words = []\n",
        "\n",
        "    # Calc Total Freq (not including noise words)\n",
        "    word_freq_sum = 0\n",
        "\n",
        "    for key in collection.documents.keys():\n",
        "        document = collection.documents[key]\n",
        "        for word in document.word_freq.keys():\n",
        "            \n",
        "            # Calc Highest Freqs\n",
        "            if word not in noise_word_array:\n",
        "                \n",
        "                # Increment Total Freq\n",
        "                word_freq_sum += document.word_freq[word]\n",
        "                \n",
        "                # Increment distinct word count if word is not already accounted for\n",
        "                if word not in all_words:\n",
        "                    distinct_words_sum += 1\n",
        "                    all_words.append(word)\n",
        "                \n",
        "                # See If Word Has Highest Freq\n",
        "                if document.word_freq[word] > highest_freq_word[1]:\n",
        "                    highest_freq_word[0] = word\n",
        "                    highest_freq_word[1] = document.word_freq[word]\n",
        "\n",
        "    collection.serialize(\"document_collection.pkl\")\n",
        "\n",
        "    print(f\"Total Word Count: {word_freq_sum}\")\n",
        "    print(f\"Distinct word count: {distinct_words_sum}\")\n",
        "    print(f\"Highest raw frequency: {highest_freq_word[1]}\")\n",
        "    print(f\"Most frequent word: {highest_freq_word[0]}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# Example usage:\n",
        "# vector = TextVector()\n",
        "# vector.add_word(\"hello\")\n",
        "# vector.add_word(\"world\")\n",
        "# vector.add_word(\"hello\")\n",
        "# vector.add_word(\"python\")\n",
        "# print(\"Raw vector entry set:\", vector.getRawVectorEntrySet())\n",
        "# print(\"Contains 'hello':\", vector.contains(\"hello\"))\n",
        "# print(\"Frequency of 'hello':\", vector.getRawFrequency(\"hello\"))\n",
        "# print(\"Total word count:\", vector.getTotalWordCount())\n",
        "# print(\"Distinct word count:\", vector.getDistinctWordCount())\n",
        "# print(\"Highest raw frequency:\", vector.getHighestRawFrequency())\n",
        "# print(\"Most frequent word:\", vector.getMostFrequentWord())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ezh4hAjcAzus"
      },
      "source": [
        "You just created your non-machine learning model: ***document_collection*** which can be used for data mining a text corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNaVif2gARPU"
      },
      "source": [
        "### Helper code on how to use pickel model/ Advantage of saving your model ###\n",
        "\n",
        "The document_collection.pkl file contains a serialized version of the DocumentCollection object, which represents a collection of documents along with their text data and associated statistics.\n",
        "\n",
        "After serializing the DocumentCollection object to the document_collection.pkl file, you can perform various operations with it, such as:\n",
        "\n",
        "Loading: You can load the serialized DocumentCollection object back into memory in another Python script or session. This allows you to access the document data and perform further analysis or processing without needing to re-parse the original text data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18KysN4rAXwV"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjoIQwRjAfUO"
      },
      "outputs": [],
      "source": [
        "with open(\"document_collection.pkl\", \"rb\") as f:\n",
        "    collection = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XmVVfbiAhoA"
      },
      "source": [
        "Once loaded, you can analyze the document data stored in the DocumentCollection object. You can calculate statistics, perform text mining tasks, generate visualizations, or extract insights from the document corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ej0ZHFlJAjx_"
      },
      "outputs": [],
      "source": [
        "# Calculate total word count of the collection\n",
        "total_word_count = sum(text_vector.getTotalWordCount() for text_vector in collection.documents.values())\n",
        "\n",
        "print(total_word_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHacqmybAPj8"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
